{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installing twint\n",
    "Installing twint will install all related packages (like numpy,etc) that makes twint function properly.\n",
    "\n",
    "### Upgrading twint\n",
    "For those who already have twint and wish to upgrade it due to certain functionality not working, or any other reasons, run the uninstall command first.\n",
    "\n",
    "Otherwise, for a fresh install, you may skip running the code below and start from the second cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: twint 2.1.20\n",
      "Uninstalling twint-2.1.20:\n",
      "  Successfully uninstalled twint-2.1.20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip3 uninstall twint --yes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/twintproject/twint.git@origin/master\n",
      "  Cloning https://github.com/twintproject/twint.git (to revision origin/master) to /private/var/folders/1j/w13pmgjd6r59876tb75jxls80000gn/T/pip-req-build-_lb73rkj\n",
      "  Running command git clone -q https://github.com/twintproject/twint.git /private/var/folders/1j/w13pmgjd6r59876tb75jxls80000gn/T/pip-req-build-_lb73rkj\n",
      "\u001b[33m  WARNING: Did not find branch or tag 'origin/master', assuming revision or ref.\u001b[0m\n",
      "  Running command git checkout -q origin/master\n",
      "Requirement already satisfied, skipping upgrade: aiohttp in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (3.7.3)\n",
      "Requirement already satisfied, skipping upgrade: aiodns in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (2.0.0)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (4.8.0)\n",
      "Requirement already satisfied, skipping upgrade: cchardet in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (2.1.7)\n",
      "Collecting dataclasses\n",
      "  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied, skipping upgrade: elasticsearch in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (7.11.0)\n",
      "Requirement already satisfied, skipping upgrade: pysocks in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (0.25.1)\n",
      "Requirement already satisfied, skipping upgrade: aiohttp_socks in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (0.5.5)\n",
      "Requirement already satisfied, skipping upgrade: schedule in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (1.0.0)\n",
      "Requirement already satisfied, skipping upgrade: geopy in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (2.1.0)\n",
      "Requirement already satisfied, skipping upgrade: fake-useragent in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (0.1.11)\n",
      "Requirement already satisfied, skipping upgrade: googletransx in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from twint==2.1.21) (2.4.2)\n",
      "Requirement already satisfied, skipping upgrade: yarl<2.0,>=1.0 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint==2.1.21) (1.6.3)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.3.0 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint==2.1.21) (19.2.0)\n",
      "Requirement already satisfied, skipping upgrade: multidict<7.0,>=4.5 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint==2.1.21) (5.1.0)\n",
      "Requirement already satisfied, skipping upgrade: async-timeout<4.0,>=3.0 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint==2.1.21) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.5 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint==2.1.21) (3.7.4.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4.0,>=2.0 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from aiohttp->twint==2.1.21) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: pycares>=3.0.0 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from aiodns->twint==2.1.21) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: soupsieve>=1.2 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from beautifulsoup4->twint==2.1.21) (1.9.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<2,>=1.21.1 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from elasticsearch->twint==2.1.21) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from elasticsearch->twint==2.1.21) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint==2.1.21) (2019.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint==2.1.21) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from pandas->twint==2.1.21) (1.17.2)\n",
      "Requirement already satisfied, skipping upgrade: python-socks[asyncio]>=1.0.1 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from aiohttp_socks->twint==2.1.21) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: geographiclib<2,>=1.49 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from geopy->twint==2.1.21) (1.50)\n",
      "Requirement already satisfied, skipping upgrade: requests in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from googletransx->twint==2.1.21) (2.22.0)\n",
      "Requirement already satisfied, skipping upgrade: idna>=2.0 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from yarl<2.0,>=1.0->aiohttp->twint==2.1.21) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.5.0 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from pycares>=3.0.0->aiodns->twint==2.1.21) (1.12.3)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->twint==2.1.21) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /Users/pcowe/opt/anaconda3/lib/python3.7/site-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint==2.1.21) (2.19)\n",
      "Building wheels for collected packages: twint\n",
      "  Building wheel for twint (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for twint: filename=twint-2.1.21-py3-none-any.whl size=38760 sha256=e07ff87bfe6cad5c2b948cca583b2cd4bd3a38fc2bdd9d8b37661cb02d9eca27\n",
      "  Stored in directory: /private/var/folders/1j/w13pmgjd6r59876tb75jxls80000gn/T/pip-ephem-wheel-cache-zjln_dgb/wheels/8d/dc/9f/74b4483d5f997036f04aec7f42bd4b3c80f04264920c368068\n",
      "Successfully built twint\n",
      "Installing collected packages: dataclasses, twint\n",
      "\u001b[33m  WARNING: The script twint is installed in '/Users/pcowe/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed dataclasses-0.6 twint-2.1.21\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Users/pcowe/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --user --upgrade git+https://github.com/twintproject/twint.git@origin/master #egg=twint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nest_asyncio\n",
      "  Downloading nest_asyncio-1.5.1-py3-none-any.whl (5.0 kB)\n",
      "Installing collected packages: nest-asyncio\n",
      "Successfully installed nest-asyncio-1.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Users/pcowe/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-0.15.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-0.15.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Users/pcowe/opt/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Packages\n",
    "Importing modules and essential packages for running this notebook :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, csv, math\n",
    "# Basic utilities for processing data in python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Open source Twitter intelligence tool\n",
    "import twint\n",
    "# Twitter authentication client\n",
    "from twitterclient import get_twitter_client\n",
    "# Twitter API client\n",
    "import tweepy\n",
    "from tweepy import Cursor\n",
    "# Removes runtimeError in jupyterlab -> This event loop is already running\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Find network actors using twint\n",
    "Use the cell below to gather information/data about different search queries, hasthags, etc to identify which actors and conversations you wish to study.\n",
    "\n",
    "All data from this section will be stored in '/search queries' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Configure twint query parameters to find all users talking about FSL\n",
    "\n",
    "qp = twint.Config()\n",
    "qp.Search = \"French as a second language\"\n",
    "qp.Output = \"data/search queries/fsl-test.csv\"\n",
    "qp.Store_csv = True\n",
    "\n",
    "# Run twint on configured qp\n",
    "\n",
    "twint.run.Search(qp)\n",
    "print(\"Running search query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to find network actors (Domain knowledge)\n",
    "\n",
    "A group of researchers with domain knowledge about FSL communities have shared some of their finidngs. I have discovered new users who are quite relevant to our analysis that I will manually add to our list\n",
    "\n",
    "### Frequently mentioned users in posts discussing FSL (Research conducted by Liam Bekirsky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "liams_userlist = ['tdsb_fsl', 'tdsbvs', 'Natasha_Faroogh', 'scholasticCDA', 'OEEO_OCT', 'OCT_OEEO', 'sarnott_uottawa', 'EducLang', 'mimi_masson', 'MonsieurSteve1', 'CASLT_ACPLS', 'CPFontario', 'sdvlil', 'CecileRobertso5', 'Alex076', 'HDSBFSL', 'aliceF11', 'transformingfsl', 'ginagkozak1', 'edu_chantal', 'MmeCoulson', 'EricKeunne', 'FrenchStreetCa', 'CPFNational', 'stephendavisSK', 'mmecarr', 'Cinefranco', 'broadwayprofe', 'MlleMouland', 'booklamations', 'ClasseMHussain', 'SylviaJudek', 'MmeFagnan1', 'joel_055', 'RECRAE_RSEKN', 'KNAER_RSEKN', 'JoseeLeBouthill', 'cabirees', 'eostaffdevnet', 'DeniseAndre613']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Build network\n",
    "In this step we need to identify all actors that we will include in the network we are going to study. To do this, we will extract usernames from all the user names we extracted in the /search queries folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables\n",
    "MAX_FRIENDS = 15000\n",
    "client = get_twitter_client()\n",
    "max_pages = math.ceil(MAX_FRIENDS/ 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup paginate function\n",
    "def paginate(items, n):\n",
    "    \"\"\"\n",
    "    Generate n-sized chunks from Items\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(0, len(items), n):\n",
    "        yield items[i:i+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open csv file to extract column names\n",
    "with open('data/search queries/fsl-test.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    columnNames = []\n",
    "    for row in csv_reader:\n",
    "        columnNames.append(row)\n",
    "        #break after reading first row\n",
    "        break\n",
    "columnNames = columnNames[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo: Add code to extract list of username from multiple csv files\n",
    "\n",
    "data = pd.read_csv('data/search queries/fsl-test.csv', names=columnNames)\n",
    "usernames = data.username.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1paulowe\n"
     ]
    }
   ],
   "source": [
    "print(usernames[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Username trimming\n",
    "Because there is simply toooooo many tweets that meet our keyword criteria, we would like to intorduce an additional threshold value for us to consider a username. \n",
    "\n",
    "Most usernames, if not trimmed, are irrelevant to our search and may actually appear in our dataset only by chance and not because they regularly participate in the FSL discussion.\n",
    "\n",
    "Our threshold cutoff would therefore be:  participations in the FSL discussion for us to consider expanding into any users network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many users are in our list\n",
    "print(len(usernames))\n",
    "\n",
    "# Find duplicates\n",
    "import collections\n",
    "dupli = [item for item, count in collections.Counter(usernames).items() if count > 1]\n",
    "\n",
    "# For each duplicate find out how many times it actually occurs in our usernames list -  this will identify \"active users in the FSL discussion\"\n",
    "shortlist = \"data/usershortlist.csv\"\n",
    "fieldnames = ['Username', 'Count']\n",
    "shortlisted = []      \n",
    "with open(shortlist, 'a') as f:\n",
    "    for u in dupli:\n",
    "        csv_writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        count = usernames.count(u)\n",
    "        #print(u, count)\n",
    "        csv_writer.writerow({'Username': u, 'Count': count})\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semi-automatic trimming of shortlist dataset\n",
    "Manually trimmed dataset of duplicates\n",
    "Kept usernames with 40+ occurences of the queried phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7895\n",
      "sdvlil 3\n",
      "petawawamfrc 2\n",
      "lily_and_googie 5\n",
      "fourre_tout 3\n",
      "ducottawa 2\n",
      "gmdiversity 27\n",
      "afrikashabbazz 2\n",
      "batatahelwe 2\n",
      "bonjouralberta 2\n",
      "fiedconsulting 37\n",
      "ilob_olbi 2\n",
      "uottawa 4\n",
      "67capt_canuck 3\n",
      "nevin_thompson 2\n",
      "kloogame 82\n",
      "ldakingston1 3\n",
      "ocolcanada 4\n",
      "sharonzilinsk 2\n",
      "caslt_acpls 4\n",
      "jodidevries 2\n",
      "uottawaedu 7\n",
      "afrodykee 3\n",
      "pauldallison 3\n",
      "susanforrester1 3\n",
      "mvlibertas 8\n",
      "ida_ontario 2\n",
      "citimmcanada 10\n",
      "torontopiac 2\n",
      "pacinter 2\n",
      "francocnac 2\n",
      "intedtoday 18\n",
      "palliserheights 5\n",
      "school_pushkino 89\n",
      "rodsilliphant 4\n",
      "cap741776 5\n",
      "kerravon86 2\n",
      "jenny_nice 2\n",
      "skyprincess88 2\n",
      "collegeboreal 2\n",
      "chantal_mme 3\n",
      "nouziecom 2\n",
      "kirstywoolven 2\n",
      "rastasack 2\n",
      "canadavacancies 2\n",
      "oectaprov 6\n",
      "davekeating 3\n",
      "coltaine777 2\n",
      "our_languages 8\n",
      "frenchstreetca 9\n",
      "durhamcatholic 9\n",
      "bevkj 4\n",
      "glendoncampus 2\n",
      "fl0wer91 2\n",
      "dianebernaerts 2\n",
      "edelweiss_erwin 2\n",
      "alloccamirella 2\n",
      "lovellpropguru 2\n",
      "ymcaedm 2\n",
      "ymcagrprairie 2\n",
      "ymcawoodbuffalo 2\n",
      "ymcareddeer 2\n",
      "sregoczei 5\n",
      "capt_scarlett 2\n",
      "fenelontownship 2\n",
      "uottawaarts 3\n",
      "cpfnational 9\n",
      "cpfontario 12\n",
      "uofsudbury 2\n",
      "bourassa1963 4\n",
      "mochateee 2\n",
      "councilofed 4\n",
      "ccmec 9\n",
      "edupointlimited 2\n",
      "arkulari 2\n",
      "petiteaitza 2\n",
      "remotelearn 2\n",
      "haltondsb 6\n",
      "amandatamsinml 6\n",
      "aliciafish 2\n",
      "brushfire53 2\n",
      "premierecanada 2\n",
      "matthieuprunier 3\n",
      "yorkueducation 3\n",
      "whatwhohowhen 2\n",
      "garyallanschool 3\n",
      "alcdsb 9\n",
      "paulinestirlin3 30\n",
      "josephsonadam1 2\n",
      "kguilaine 2\n",
      "ackaminski 2\n",
      "mikecommito 2\n",
      "limestonedsb 4\n",
      "ne_children 4\n",
      "aphexgwyn 2\n",
      "hwdsb 5\n",
      "mathieu_hetu 2\n",
      "ghosthermione 2\n",
      "mme_dibenz 10\n",
      "mflchat 2\n",
      "tttediting 2\n",
      "centerhyuka 2\n",
      "brosina91 2\n",
      "arianna483 2\n",
      "robhoadley 2\n",
      "edosianorchid 2\n",
      "omniarz 2\n",
      "mmenero 2\n",
      "justjuning 2\n",
      "fabpaule 2\n",
      "ayrshirebog 2\n",
      "kristinaburns08 4\n",
      "thepoetarefke 3\n",
      "joshalexcairo 2\n",
      "foxhollowcraft 4\n",
      "chrislowndes 2\n",
      "mirandamccorm17 2\n",
      "gunjanm 2\n",
      "ellyhawk 2\n",
      "dfnclesslou 2\n",
      "aureliacotta 2\n",
      "marycruden 7\n",
      "njschooljobs 3\n",
      "algonquincolleg 7\n",
      "cpf_vernon 2\n",
      "cuhsandra 2\n",
      "altavistagoogle 2\n",
      "thelauriedr 3\n",
      "humber_fla 3\n",
      "bibo784 2\n",
      "sncdsb 31\n",
      "chinmayi 2\n",
      "nearpup 4\n",
      "yvonne_fiala 2\n",
      "adibene1970 4\n",
      "briasoleil 3\n",
      "shelleylaskin 6\n",
      "ipschool 2\n",
      "durhamdsb 16\n",
      "iisle_epsb 5\n",
      "smile_jhs 2\n",
      "i81b4i8uicu812 2\n",
      "frenchspanishmb 32\n",
      "pwpsd 2\n",
      "luveiytae 2\n",
      "john_chandler 2\n",
      "secretsandnudes 2\n",
      "jessiedawnk 6\n",
      "lilmeowfairy 2\n",
      "tannerofdanorth 2\n",
      "learnquebec 2\n",
      "jillianives 2\n",
      "andkenbr 2\n",
      "npsc_schools 5\n",
      "yourgirlratbaby 3\n",
      "isblib 4\n",
      "cwhmathome 2\n",
      "netnoonco 2\n",
      "premierbhiggs 2\n",
      "travel_msw 2\n",
      "bluewaterdsb 5\n",
      "k_musgr 2\n",
      "dyslexiabc 3\n",
      "montreallisting 2\n",
      "lblovespaper 2\n",
      "schoolvickers 2\n",
      "rollinsjobs 2\n",
      "daliwchifreud 5\n",
      "soniamonette 2\n",
      "portstanleynews 2\n",
      "opsba 11\n",
      "drivandalism 2\n",
      "broadwayprofe 2\n",
      "catsinfrance 2\n",
      "wraillantclark 2\n",
      "coryjankoski 2\n",
      "bar_qu 2\n",
      "tagawthroupe 3\n",
      "samantar 2\n",
      "apathystinks 2\n",
      "c_mulroney 2\n",
      "nobleknits2 10\n",
      "rcdsb 2\n",
      "kdenkss 2\n",
      "tenhinas_rt 2\n",
      "gukkie_patootae 2\n",
      "ontspecialneeds 18\n",
      "comsense_energy 3\n",
      "alibowow 2\n",
      "skepticmohamed 2\n",
      "manusancero 4\n",
      "britishbc 2\n",
      "study_french 3\n",
      "ugdsb 15\n",
      "oscar1710_ 2\n",
      "joseelebouthill 2\n",
      "_screenhog 2\n",
      "galskdev 2\n",
      "oct_oeeo 9\n",
      "rekks_ 3\n",
      "_adropofred 2\n",
      "steve_m_parsons 2\n",
      "jiwashere 4\n",
      "langevinmartin 7\n",
      "mycanadiancarma 4\n",
      "madameaiello 5\n",
      "polyglotfiles 5\n",
      "oeeo_oct 2\n",
      "leblancpeter 2\n",
      "drschouinard 2\n",
      "paulrobwright 2\n",
      "dyslexiaon 3\n",
      "diplomix 2\n",
      "swdickey 2\n",
      "vasan40914704 2\n",
      "luther_gen 2\n",
      "onparleducation 5\n",
      "howardjrps 3\n",
      "qcgn 2\n",
      "paulcarr70 2\n",
      "cdnheritage 2\n",
      "hotrollhottakes 2\n",
      "scdsb_schools 11\n",
      "frank61pc 2\n",
      "llrc_accll 2\n",
      "languageacad1 2\n",
      "franceinboston 2\n",
      "bcs1836 11\n",
      "cecilerobertso5 2\n",
      "prettyinthin 2\n",
      "on_haaco 3\n",
      "freyaodin8 2\n",
      "nofnews_ghana 7\n",
      "princedavidosei 3\n",
      "l_paradise77 2\n",
      "frenchonskype 3\n",
      "4shade17 2\n",
      "ghanamotion 2\n",
      "hitz1039fm 2\n",
      "peacefmonline 4\n",
      "cpf_nwt 4\n",
      "bottishamvc 3\n",
      "profwebinfo 3\n",
      "chantler_jaki 2\n",
      "ipoliticsca 2\n",
      "jobs_gc 2\n",
      "educationnewsca 2\n",
      "oneducation 9\n",
      "canesbian 3\n",
      "ecole_michael 2\n",
      "hwdsbfsl 5\n",
      "alvinbuckwold 3\n",
      "linguistlist 3\n",
      "timminsdmc 3\n",
      "canadaconnect 5\n",
      "sardissecondary 16\n",
      "msf_usa 2\n",
      "crfmmfrcmtl 10\n",
      "joseemricher 2\n",
      "lasboris 2\n",
      "clssroomfreebie 2\n",
      "bilinguasing 13\n",
      "surreycollege 2\n",
      "retdherceng 2\n",
      "perfprovence 2\n",
      "employjourney 4\n",
      "ncrtopemployers 2\n",
      "tomedes 3\n",
      "bassmanrab 2\n",
      "kaylaframboise 2\n",
      "halifaxca1 3\n",
      "ccet180 5\n",
      "kitten_arms 2\n",
      "cpf_pei 2\n",
      "anis79480994 2\n",
      "elizawallace27 2\n",
      "hoccommittees 9\n",
      "davidakin 2\n",
      "mandyjmoore 3\n",
      "ramirezfsl 2\n",
      "profssilva 3\n",
      "bellevue_school 3\n",
      "ainogundan 2\n",
      "kataomi 2\n",
      "stabtrustee 2\n",
      "sbclanguagescs 2\n",
      "conted_lennox 2\n",
      "lucas_bourdon 2\n",
      "al_nw_music 12\n",
      "yzz_praetor_83 2\n",
      "lespucesuk 2\n",
      "sjscribe 2\n",
      "diversity__jobs 3\n",
      "literensics 2\n",
      "ciudadwestonnew 2\n",
      "educationghana 2\n",
      "victory_ps 2\n",
      "frajud12 2\n",
      "afro_insider 2\n",
      "nationtalk 2\n",
      "peelschools 2\n",
      "dsbonnortheast 3\n",
      "tevitweets 2\n",
      "francophoniecan 2\n",
      "bcatml 2\n",
      "antoinettedewit 3\n",
      "juuddith 2\n",
      "torontoism 2\n",
      "vulcanraven961 2\n",
      "school_gc 9\n",
      "mcgregormatter 3\n",
      "omlta 2\n",
      "nmaf30 2\n",
      "tldsb 3\n",
      "full_meals 2\n",
      "beaute081 2\n",
      "mmealexanderfi 2\n",
      "redqredt 2\n",
      "lingozing 4\n",
      "twelve_fiftyone 4\n",
      "travellersoul76 2\n",
      "legalkant 2\n",
      "willsjamsmusic 18\n",
      "creolebabybritt 2\n",
      "leslietelfer 2\n",
      "bostonmamoms 2\n",
      "planbookcom 2\n",
      "cpfhamilton 5\n",
      "ajmulford 4\n",
      "fslvincent 2\n",
      "gretchenamcc 2\n",
      "fabula 4\n",
      "arlette_ibanez 2\n",
      "ottawakiwi 2\n",
      "sjohnstonslt 3\n",
      "glorafin 2\n",
      "monsignoro 2\n",
      "pamelagough 2\n",
      "londonchildcare 3\n",
      "trabajosmontrel 54\n",
      "justadded 2\n",
      "cefa_ssm 3\n",
      "trabajos_quebec 40\n",
      "collegenordique 2\n",
      "trabajostoronto 7\n",
      "schooladvice_ca 2\n",
      "smileyrose_uk 2\n",
      "cosmindart 2\n",
      "pgracile 2\n",
      "kingstonmfrc 4\n",
      "trabajolondres 4\n",
      "boreal_toronto 4\n",
      "anglophoneeast 2\n",
      "hirekw 3\n",
      "second_language 15\n",
      "rfsso 2\n",
      "ptbocareers 13\n",
      "teachingjobsca 3\n",
      "frenchiephoto 3\n",
      "utopiaseeker 3\n",
      "rachadelasri 2\n",
      "tpl_bot 2\n",
      "pspc_spac 4\n",
      "hyderabadparent 2\n",
      "dagarneau 2\n",
      "highedcareersca 25\n",
      "ucdsb 2\n",
      "jennifer_arp 2\n",
      "simoneroliver 4\n",
      "neuvoohalifaxns 3\n",
      "lunacarry 5\n",
      "neuvooatlantaus 127\n",
      "infodouze0deux 2\n",
      "omsmontessori 2\n",
      "neuvoothanein 2\n",
      "rfpbidscanada 21\n",
      "morilibrary 2\n",
      "lfrenchfacile 2\n",
      "wycliffeuk 2\n",
      "catchajob_ca 9\n",
      "mjbiblioteca 2\n",
      "neuvoosharjahar 2\n",
      "iamdicksonbuff 5\n",
      "langcanada 4\n",
      "wkortleever 2\n",
      "kim_doc 2\n",
      "teachcarpentry 2\n",
      "etwinninguk 2\n",
      "yrdsb 4\n",
      "oshigotousa 8\n",
      "oshigotoeus 2\n",
      "kallilirette 2\n",
      "katerey523 2\n",
      "eluta_jobs 4\n",
      "edugainsedu 2\n",
      "wamlahore 2\n",
      "pullapaul 3\n",
      "ottcatholicsb 3\n",
      "slmahmadkashif 5\n",
      "dionnecharles 2\n",
      "kmlaperle 2\n",
      "idellotfo 6\n",
      "loveiysophia 2\n",
      "neuvoocomca 61\n",
      "bowmoresc 2\n",
      "teacherspet 2\n",
      "yrdsb_lrsic 2\n",
      "gppsd2357 2\n",
      "tdottawa 2\n",
      "glblcanuck 2\n",
      "msrunmadmaxrun 2\n",
      "settlement_org 2\n",
      "mme_henderson 3\n",
      "lidiabila 2\n",
      "cteachr 2\n",
      "bythewater21 4\n",
      "frenchforlife2 2\n",
      "gtatopemployers 2\n",
      "familyjobs 2\n",
      "linguawise 2\n",
      "cdnhomeschooler 3\n",
      "rawlinsonsac 2\n",
      "canjobsourcer 18\n",
      "majachere 40\n",
      "xcherrytrix 2\n",
      "johnbenjamins 2\n",
      "findpdfbooks 2\n",
      "nancy_s_mundt 3\n",
      "wbrl_ab 2\n",
      "everythingftmac 2\n",
      "evangelmontreal 2\n",
      "peoplesbalance 3\n",
      "langcanada_ca 9\n",
      "gsaaees 2\n",
      "davidrankin 2\n",
      "piirate_ 3\n",
      "aniekwon 2\n",
      "usainteannefls 2\n",
      "oiselibrary 3\n",
      "donvrooman 2\n",
      "hffins 4\n",
      "jenbarron 2\n",
      "lil_bilinguals 4\n",
      "fille_valentine 2\n",
      "ljpschool 9\n",
      "livamour 2\n",
      "bagginsbilbo_ 2\n",
      "danielisrunning 2\n",
      "bilingualguides 7\n",
      "amandamatchett1 2\n",
      "oakvillebeaver 2\n",
      "tadaudiobooks 2\n",
      "attyannaa 2\n",
      "uottawafuture 2\n",
      "montrealbizcaf 6\n",
      "torontojoblist 3\n",
      "bizcaf 4\n",
      "marina_dufour 36\n",
      "fionapond 2\n",
      "cmorecruitment 6\n",
      "communicaidlang 10\n",
      "winnepegjobs 2\n",
      "algonquinpem 3\n",
      "tootleootle 3\n",
      "barbiegirl13xo 2\n",
      "_timng 2\n",
      "mapucebilingual 7\n",
      "umontreal_news 5\n",
      "frenchjobsusa 2\n",
      "iljobfinder 2\n",
      "edmontonbizcaf 2\n",
      "gladtohate 2\n",
      "jobsincanada_ 7\n",
      "oxfordedmfl 2\n",
      "careerbacon 4\n",
      "carolinehetu 2\n",
      "lucimcq 2\n",
      "liselynn_ 2\n",
      "stephenfgordon 3\n",
      "sarah_bo_bera 2\n",
      "zillionjobs 11\n",
      "atlanticcareer 2\n",
      "rayman25 3\n",
      "lindsblog 2\n",
      "yeahbuddiee13 2\n",
      "educated_travel 2\n",
      "kikicontessa 3\n",
      "collabrone 3\n",
      "redraccoon1 2\n",
      "teachingesljobs 19\n",
      "naochimoshiron 5\n",
      "jedisct1 2\n",
      "edsitement 2\n",
      "englishmadefun 11\n",
      "eugenejam 2\n",
      "rajeevsahadevan 5\n",
      "furnituresstore 5\n",
      "lexxioca 4\n",
      "cbeconomicn 3\n",
      "manitoulinmagic 3\n",
      "jobbank_canada 21\n",
      "summertimep 2\n",
      "dgfc_umoncton 3\n",
      "junhyungs 2\n",
      "can_headlines 2\n",
      "languageagogo 14\n",
      "ez_spanish 2\n",
      "onclassifieds 4\n",
      "job_less_info 4\n",
      "greightcompany 3\n",
      "findtorontojobs 2\n",
      "articlezdir 2\n",
      "languagebuster 2\n",
      "nooralk16 2\n",
      "chrisstrangio 2\n",
      "0englishgrammar 2\n",
      "mairinaseer 2\n",
      "ventaschool 2\n",
      "nailaj 2\n",
      "halifaxjobforce 4\n",
      "tesoldan 2\n",
      "tomwilson23 2\n",
      "auntieto2 5\n",
      "freeadscanada 11\n",
      "amacdanny 2\n",
      "ccampbel14 2\n",
      "trentonseltz 3\n",
      "timtom202 2\n",
      "thehrjob 3\n",
      "sillytink 3\n",
      "ezufelt 2\n",
      "yamasas 3\n",
      "joshmatlow 2\n",
      "gorickng 2\n",
      "lethbridgejobs 2\n",
      "padawanryan 2\n",
      "nwtjobs 2\n",
      "michlerish 2\n"
     ]
    }
   ],
   "source": [
    "#print(len(dupli))\n",
    "#print(len(liams_userlist))\n",
    "\n",
    "#print([item for item, count in collections.Counter(usernames).items() if count > 1])\n",
    "# How many duplicates are in our list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Edges\n",
    "\n",
    "In the next part I will create directed edges from our network of users. This is directed because we are goimg to represent followers/following relationships for each user in our general group of users.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweepy error occured: [{'message': 'Rate limit exceeded', 'code': 88}]. Skipped adding user: 1paulowe\n"
     ]
    }
   ],
   "source": [
    "# make sure you are not repeating already scraped users. You will waste time\n",
    "# for this, check your network-progress list for users you already have scraped and remove them from usernames \n",
    "\n",
    "# load updated username list before you continue to build your network \n",
    "\n",
    "progress_params = ['explored_users', 'skipped_users']\n",
    "network_progress = pd.read_csv('data/network/network-progress2.csv', names=progress_params)\n",
    "explored_users = network_progress.explored_users.tolist()\n",
    "skipped_users = network_progress.skipped_users.tolist()\n",
    "# ignoring the nan's and column names, len(explored) + len(skipped) = len(usernames) \n",
    "unames = [ x for x in usernames if x not in (explored_users + skipped_users)]\n",
    "\n",
    "#loop through all users and write their followers to csv\n",
    "try:\n",
    "    \n",
    "    for user in unames:\n",
    "        \n",
    "\n",
    "        fname = \"data/network/network-edges2.csv\"\n",
    "        fieldnames = ['Source', 'Target']\n",
    "        with open(fname, 'a') as f:\n",
    "\n",
    "            csv_writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "\n",
    "            # get a user's followers list\n",
    "            for followers in Cursor(client.followers_ids, screen_name=user).pages(max_pages):\n",
    "                for chunk in paginate(followers, 100):\n",
    "                    userFollowers = client.lookup_users(user_ids=chunk)\n",
    "\n",
    "                    for follower in userFollowers:\n",
    "\n",
    "                        #print(len(users))\n",
    "\n",
    "                        #print(follower.screen_name)\n",
    "                        csv_writer.writerow({'Source': follower.screen_name, 'Target': user})\n",
    "\n",
    "                    # f.write(json.dumps(user._json)+\"\\n\")\n",
    "                if len(followers) == 5000:\n",
    "                    print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
    "                    time.sleep(60)\n",
    "\n",
    "            # get a user's following list\n",
    "            for friends in Cursor(client.friends_ids, screen_name=user).pages(max_pages):\n",
    "\n",
    "                for chunk in paginate(friends, 100):\n",
    "                    userFollowing = client.lookup_users(user_ids=chunk)\n",
    "                    for followed in userFollowing:\n",
    "                        csv_writer.writerow({'Source': user, 'Target': followed.screen_name})\n",
    "                if len(friends) == 5000:\n",
    "                    print(\"More results available. Sleeping for 60 seconds to avoid rate limit\")\n",
    "                    time.sleep(60)\n",
    "\n",
    "            f.close()\n",
    "            \n",
    "            # Update network-progress\n",
    "            with open('data/network/network-progress2.csv', 'a') as nprog:\n",
    "                nprog_updater = csv.DictWriter(nprog, fieldnames=progress_params)\n",
    "                nprog_updater.writerow({'explored_users': user, 'skipped_users': ' '})\n",
    "                nprog.close()\n",
    "            \n",
    "except tweepy.TweepError as err:\n",
    "    \n",
    "    \n",
    "    print(\"Tweepy error occured: {}. Skipped adding user: {}\".format(err, user))\n",
    "    with open('data/network/network-progress2.csv', 'a') as nprog:\n",
    "        nprog_updater = csv.DictWriter(nprog, fieldnames=progress_params)\n",
    "        nprog_updater.writerow({'explored_users': ' ', 'skipped_users': user})\n",
    "        nprog.close()\n",
    "    pass\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Nodes\n",
    "\n",
    "In the next part I will create nodes from our network actors. \n",
    "\n",
    "### What I did\n",
    "Used excel to find all unique names from my edge-list.\n",
    "### Optional\n",
    "You can add code instead find unique usernames and also you can include specific node attributes to enhance network visualization in gephi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Send data to Gephi\n",
    "In this step download gephi. It is an open source network analysis software\n",
    "\n",
    "Next, watch a few youtube videos to understand how you can use gephi to upload your network nodes and edges.\n",
    "\n",
    "Learn how to visualize your network on gephi \n",
    "\n",
    "![Network clusters](network-analysis/network-clusters.png)\n",
    "\n",
    "\n",
    "Learn how to run algorithms such as PageRank, Eigenvector centrality and clustering algorithms on your network\n",
    "\n",
    "Here's an example of community detection based on pagerank\n",
    "\n",
    "![Network clusters](network-analysis/network-communitydetect.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Infographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
